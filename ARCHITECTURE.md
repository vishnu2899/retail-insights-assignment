# Retail Insights Assistant – Architecture & 100GB+ Scaling Design

This document explains how the Retail Insights Assistant is put together today (for the assignment) and how the same design can scale to datasets that are 100GB and beyond. You can treat this as the “script” that turns directly into slides.

## 1. High-Level System Architecture

- **User Interface (Streamlit)**  
  - Simple web UI where the user uploads CSV / report files.  
  - Lets the user choose a mode: *Summarization* or *Conversational Q&A*.  
  - Renders data previews, generated summaries, and Q&A responses.

- **Multi-Agent Orchestration (Python)**  
  - **Language-to-Query Agent** – takes a natural language question and the table schema and turns it into a DuckDB SQL query.  
  - **Data Extraction Agent** – runs that SQL against the analytical store and packages the result into a compact summary.  
  - **Validation Agent** – checks if the SQL result really answers the question and converts it into a clear user-facing answer (or a safe refusal).  
  - **Summarization Flow** – skips SQL entirely and calls the LLM directly using dataset profile + aggregates.

- **LLM Layer (OpenAI via LangChain)**  
  - Uses a chat model (e.g., `gpt-4o-mini`) via `langchain-openai`.  
  - Separate prompt templates for the main tasks: summarization, language-to-SQL, and validation.  
  - For conversational Q&A, previous turns from the UI are passed as chat history so the model can keep track of context.

- **Data Layer (DuckDB + Pandas)**  
  - For the assignment scale, the data lives in an in-memory DuckDB database.  
  - Uploaded CSV / Excel / JSON is loaded into a single `sales` table.  
  - The SQL generated by the Language-to-Query agent is executed against this table.  
  - The same layer also computes lightweight profiling and aggregates used by the summarization flow.

## 2. Query-Response Pipeline (Example)

To make the flow concrete, consider this example question:

> "Which category saw the highest YoY growth in Q3 in the North region?"

The end-to-end steps look like this:

1. **Language-to-Query Agent**  
   - Reads the `sales` table schema (column names and types).  
   - Plans the right level of aggregation and filters (e.g., group by category, filter `region = 'North'`, restrict to Q3 months, compute year-on-year change).  
   - Produces a DuckDB-compatible SQL query plus metadata as JSON: `{ sql, reasoning, confidence }`.

2. **Data Extraction Agent**  
   - Executes the generated SQL via DuckDB, returning a Pandas DataFrame.  
   - Converts the DataFrame into a compact summary: row count, column list, and the top 20 rows as a preview.

3. **Validation Agent**  
   - Receives the original question, the SQL, and the result summary.  
   - Checks whether the result actually answers the question (for example, whether there is at least one row and the expected columns are present).  
   - Returns JSON: `{ is_valid, confidence, critique, user_facing_answer }`.

4. **UI Response**  
   - Shows the `user_facing_answer` back to the user in natural language.  
   - Optionally, the app can also surface a debug panel containing all intermediate agent outputs for transparency.

## 3. Summarization Mode

In summarization mode, the system is not trying to answer a very specific question. Instead, it produces an executive-style overview of the dataset.

- The data layer computes a dataset **profile** (row count, column list) and a few **aggregates** (for example, total sales and a monthly trend if an amount and date column exist).  
- These structures are fed into a tailored summarization prompt asking the LLM to write a 3–6 bullet, business-friendly summary.  
- No SQL generation is needed here; the model reasons purely over the high-level statistics it is given.

## 4. 100GB+ Scaling Design

When the dataset grows into the 100GB+ range, the same logical architecture still applies, but the implementation needs to move from in-memory analytics to a more robust data platform. This section outlines how each layer evolves.

### 4.1 Data Engineering & Preprocessing

- **Ingestion**  
  - Use distributed processing frameworks such as **PySpark** or **Dask** to ingest raw CSV / transactional data from object storage (S3, ADLS, GCS).  
  - Orchestrate batch jobs with tools like Airflow or Databricks Jobs to process daily or hourly increments.  
  - Optionally, add streaming via Kafka / Kinesis feeding Spark Structured Streaming to maintain near-real-time tables.

- **Cleaning & Transformation**  
  - Standardize core entities: dates, product hierarchy, region codes, channel names, etc.  
  - Derive useful features such as `order_month`, `fiscal_quarter`, `year`, basket size, and discount %.  
  - Persist data in **columnar formats (Parquet/Delta)** and partition by date and region to keep large queries efficient.

- **Curated Analytical Layer**  
  - Build well-modeled fact tables (for example `fact_sales`) and dimension tables (`dim_product`, `dim_customer`, `dim_region`).  
  - Apply data quality checks (Great Expectations / Deequ) before new partitions are exposed to downstream consumers.

### 4.2 Storage & Indexing

- **Data Lake / Lakehouse**  
  - Store all raw and processed data in S3 / ADLS / GCS using Parquet or Delta format.  
  - Use **Delta Lake** or a similar technology to get ACID guarantees, time travel, and safe schema evolution.

- **Cloud Data Warehouse**  
  - Expose the curated Parquet/Delta data through **BigQuery, Snowflake, or Databricks SQL**.  
  - This warehouse becomes the main analytical engine that the agents talk to, replacing the local DuckDB instance.

- **Analytical Indexing**  
  - Partition tables by `year`, `month`, `region`, `channel` to reduce scan size.  
  - Use clustering or sorting keys on `product_category` and `sku_id` for selective queries.  
  - Create materialized views for common access patterns, such as monthly revenue by region.

### 4.3 Retrieval & Query Efficiency

- **Metadata-based Filtering**  
  - Introduce a lightweight **intent classifier** agent that inspects the question and infers the grain:  
    - time-based (YoY, QoQ, trends),  
    - geography-based (region, city, country),  
    - product-based (category, sub-category, SKU).  
  - Map this intent to warehouse partition filters so each query touches only the relevant slices of data.

- **Vector Indexing / RAG**  
  - Build a **semantic index** over key documentation:  
    - schema and column descriptions,  
    - metric definitions and business glossaries,  
    - historical successful queries.  
  - Use FAISS / Pinecone / ChromaDB to retrieve the most relevant documents for a given question.  
  - Inject that retrieved context into the Language-to-Query agent’s prompt so it can generate more accurate SQL and choose the right metrics.

- **RAG Over Aggregates**  
  - Precompute and store aggregated tables (for example, monthly sales by region and category).  
  - Expose these as separate logical tables and steer the LLM to use them for most analytics questions, avoiding unnecessary full-detail scans.

### 4.4 Model Orchestration at Scale

- **LLM API Management**  
  - Place an LLM gateway in front of the underlying models (e.g., Azure OpenAI, Vertex AI, or a self-hosted gateway) with rate limiting, logging, and access control.  
  - Manage **prompt templates** centrally and version them; define clear system prompts per agent type.

- **Caching**  
  - Cache expensive or repeated computations, such as:  
    - question → SQL mappings,  
    - SQL → result summaries,  
    - question + context → final answer.  
  - Use Redis or an in-memory cache to reduce both LLM calls and warehouse load for popular queries.

- **Orchestration Framework**  
  - As flows get more complex, move from ad-hoc Python orchestration to frameworks like **LangGraph, CrewAI, or AutoGen**.  
  - Model each agent as a node in a graph and add branches for fallbacks (for example, if validation fails, re-plan the query or fall back to a higher-level aggregation).

- **Cost & Latency Controls**  
  - Use smaller / faster models for routine steps like language-to-SQL and validation, keeping larger models for ambiguous or high-value questions.  
  - Apply **token budgeting** and truncate unneeded context to keep prompt sizes under control.  
  - Batch non-urgent tasks (such as daily executive summaries) into off-peak windows.

### 4.5 Monitoring & Evaluation

- **Metrics**  
  - **Accuracy / Quality**:  
    - manual review of a sample of answers,  
    - automated checks on SQL correctness and data constraints.  
  - **Latency**:  
    - end-to-end response time seen by the user,  
    - per-component timing (LLM calls, warehouse queries, network).  
  - **Cost**:  
    - tokens consumed per request,  
    - warehouse compute cost per query.

- **Observability**  
  - Centralize logs for the full pipeline, including:  
    - incoming questions, generated SQL, executed queries, validation outcomes, and final answers.  
  - Build dashboards (Grafana / Datadog) to monitor health, usage, and failure patterns.

- **Error Handling & Fallbacks**  
  - If validation confidence is low, the system can:  
    - re-ask the Language-to-Query agent with additional guardrails or hints,  
    - fall back to a simpler or more aggregated view of the data, or  
    - explicitly ask the user a clarification question.  
  - Add guardrails on query generation to avoid accidentally triggering very expensive full-table scans (for example, maximum scan size, required filters).