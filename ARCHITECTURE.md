# Retail Insights Assistant – Architecture & 100GB+ Scaling Design

This document provides a written version of what you can convert into slides for the assignment presentation.

## 1. High-Level System Architecture

- **User Interface (Streamlit)**  
  - Upload CSV / report files.  
  - Select mode: *Summarization* or *Conversational Q&A*.  
  - Displays previews, summaries, and Q&A responses.

- **Multi-Agent Orchestration (Python)**  
  - **Language-to-Query Agent** – converts natural language questions into SQL.  
  - **Data Extraction Agent** – runs SQL over the analytical store and returns structured results.  
  - **Validation Agent** – checks whether the results answer the question and produces a user-facing answer.  
  - **Summarization Flow** – directly prompts the LLM over dataset profile + aggregates.

- **LLM Layer (OpenAI via LangChain)**  
  - Chat-based model (e.g., `gpt-4o-mini`) accessed through `langchain-openai`.  
  - Prompt templates for: summarization, language-to-SQL, and validation.  
  - Conversation context maintained by passing prior turns from the UI (chat history).

- **Data Layer (DuckDB + Pandas)**  
  - Small-scale: in-memory DuckDB database.  
  - Uploaded CSV / Excel / JSON is loaded into a `sales` table.  
  - SQL generated by the agent is executed against DuckDB.  
  - Lightweight profiling + aggregates for summarization.

## 2. Query-Response Pipeline (Example)

1. **User question**:  
   "Which category saw the highest YoY growth in Q3 in the North region?"

2. **Language-to-Query Agent**:  
   - Reads schema (columns + types).  
   - Generates a DuckDB-compatible SQL over `sales`, e.g. grouping by category, filtering region = 'North' and Q3 months, computing YoY change.  
   - Returns JSON: `{ sql, reasoning, confidence }`.

3. **Data Extraction Agent**:  
   - Executes SQL via DuckDB and obtains a result DataFrame.  
   - Produces compact summary: row count, columns, top 20 rows.

4. **Validation Agent**:  
   - Takes question + SQL + result summary.  
   - Checks if the result actually contains the answer (e.g., at least one row, expected columns present).  
   - Returns JSON: `{ is_valid, confidence, critique, user_facing_answer }`.

5. **UI Response**:  
   - Shows `user_facing_answer` plus optional debug view of all intermediate agent outputs.

## 3. Summarization Mode

- Compute dataset **profile** (row count, columns) and **aggregates** (e.g., total sales, monthly trend).  
- Feed both into a summarization prompt asking the LLM to produce a 3–6 bullet executive summary.  
- No SQL generation is required here; instead, the LLM reasons over the aggregate stats.

## 4. 100GB+ Scaling Design

When data grows beyond 100GB, the architecture evolves as follows.

### 4.1 Data Engineering & Preprocessing

- **Ingestion**  
  - Use distributed processing such as **PySpark** or **Dask** to ingest raw CSV / transactional data from object storage (e.g., S3, ADLS, GCS).  
  - Batch jobs scheduled via Airflow / Databricks Jobs to process daily or hourly increments.  
  - Optional streaming: Kafka / Kinesis → Spark Structured Streaming to maintain near-real-time tables.

- **Cleaning & Transformation**  
  - Standardize schema (dates, product hierarchy, region codes).  
  - Derive features: `order_month`, `fiscal_quarter`, `year`, basket size, discount %, etc.  
  - Write data in **columnar formats (Parquet/Delta)** partitioned by date/region for efficient queries.

- **Curated Analytical Layer**  
  - Build fact tables (`fact_sales`) and dimension tables (`dim_product`, `dim_customer`, `dim_region`).  
  - Enforce quality checks (Great Expectations / Deequ) before publishing new partitions.

### 4.2 Storage & Indexing

- **Data Lake / Lakehouse**  
  - Store all raw + processed data in S3 / ADLS / GCS in Parquet/Delta format.  
  - Use **Delta Lake** or similar for ACID, time travel, and schema evolution.

- **Cloud Data Warehouse**  
  - Expose same Parquet/Delta data through **BigQuery / Snowflake / Databricks SQL**.  
  - This becomes the main analytical engine used by the agents instead of in-memory DuckDB.

- **Analytical Indexing**  
  - Partition by `year`, `month`, `region`, `channel`.  
  - Use clustering / sorting on `product_category`, `sku_id` for selective queries.  
  - Materialized views for high-traffic queries (e.g., monthly regional revenue).

### 4.3 Retrieval & Query Efficiency

- **Metadata-based Filtering**  
  - Before generating SQL, a lightweight **intent classifier** agent determines the grain:  
    - time-based (YoY, QoQ, trend)  
    - geography-based (region, city, country)  
    - product-based (category, sub-category, SKU)  
  - Map this to partition filters in the SQL so only relevant slices are scanned.

- **Vector Indexing / RAG**  
  - Build a **semantic index** of:  
    - schema documentation (column descriptions, metric definitions),  
    - business glossaries and historical queries.  
  - Use FAISS / Pinecone / ChromaDB to retrieve the most relevant docs for a user question.  
  - Inject retrieved context into the **Language-to-Query Agent** prompt to improve SQL generation and metric correctness.

- **RAG Over Aggregates**  
  - Precompute and store aggregated tables (e.g., monthly sales by region/category).  
  - Expose them as separate logical tables and guide the LLM to use them for most questions, avoiding full-detail scans.

### 4.4 Model Orchestration at Scale

- **LLM API Management**  
  - Use an LLM gateway (e.g., Azure OpenAI / Vertex AI / self-hosted) with rate limiting and logging.  
  - **Prompt templates** versioned and stored centrally; use explicit system prompts for each agent.

- **Caching**  
  - Cache:  
    - question → SQL mappings (for repeated questions),  
    - SQL → result summaries,  
    - question + context → final answer.  
  - Use Redis / in-memory cache to reduce repeated LLM calls for popular queries.

- **Orchestration Framework**  
  - For more complex flows, switch from simple Python orchestration to **LangGraph / CrewAI / AutoGen**.  
  - Represent each agent as a node in a graph; add branches for fallback strategies (e.g., if validation fails, re-plan query or lower granularity).

- **Cost & Latency Controls**  
  - Use smaller / faster models for language-to-SQL and validation, and reserve larger models for ambiguous or high-value queries.  
  - Apply **token budgeting** and truncate unnecessary context.  
  - Batch background summarization tasks (e.g., daily executive summary) to off-peak hours.

### 4.5 Monitoring & Evaluation

- **Metrics**  
  - **Accuracy / Quality**:  
    - human rating of sampled answers,  
    - automated checks (SQL correctness, constraint checks).  
  - **Latency**:  
    - end-to-end response time,  
    - per-component timing (LLM, warehouse query, network).  
  - **Cost**:  
    - tokens per request,  
    - warehouse compute cost per query.

- **Observability**  
  - Central logging of:  
    - incoming questions, generated SQL, executed queries, validation results, and final answers.  
  - Dashboards (Grafana / Datadog) over these logs.

- **Error Handling & Fallbacks**  
  - If validation confidence is low:  
    - re-ask the Language-to-Query agent with additional constraints, or  
    - fall back to a more aggregated query, or  
    - explicitly ask the user a clarification question.  
  - Safe-guards to avoid expensive full-table scans (e.g., max scan size, query guardrails).

## 5. Slide Pointers

You can turn this document into 4–6 slides:

1. **Architecture Overview** – UI, agents, LLM, data layer.  
2. **Query Pipeline** – end-to-end flow for a single question.  
3. **Summarization Flow** – how executive summaries are generated.  
4. **100GB+ Data Platform** – ingestion, storage, indexing choices.  
5. **RAG & Orchestration at Scale** – retrieval, vector index, agent graph.  
6. **Monitoring & Improvements** – metrics, error handling, next steps.
